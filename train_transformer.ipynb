{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed05e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.10.3\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a1f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd964be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "from tokenizers import Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ffb2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'data/jd.csv', \n",
    "#     nrows=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610b3d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12613049</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 30000/annum 25K-30K negotiable</td>\n",
       "      <td>27500</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12613647</td>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244763</th>\n",
       "      <td>72705211</td>\n",
       "      <td>TEACHER OF SCIENCE</td>\n",
       "      <td>Position: Qualified Teacher Subject/Specialism...</td>\n",
       "      <td>Swindon</td>\n",
       "      <td>Swindon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>contract</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teaching Jobs</td>\n",
       "      <td>450 - 500 per week</td>\n",
       "      <td>22800</td>\n",
       "      <td>hays.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244764</th>\n",
       "      <td>72705212</td>\n",
       "      <td>TEACHER OF BUSINESS STUDIES AND ICT</td>\n",
       "      <td>Position: Qualified Teacher or NQT Subject/Spe...</td>\n",
       "      <td>Swindon</td>\n",
       "      <td>Swindon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>contract</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teaching Jobs</td>\n",
       "      <td>450 - 500 per week</td>\n",
       "      <td>22800</td>\n",
       "      <td>hays.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244765</th>\n",
       "      <td>72705213</td>\n",
       "      <td>ENGLISH TEACHER</td>\n",
       "      <td>Position: Qualified Teacher Subject/Specialism...</td>\n",
       "      <td>Swindon</td>\n",
       "      <td>Swindon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>contract</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teaching Jobs</td>\n",
       "      <td>450 - 500 per week</td>\n",
       "      <td>22800</td>\n",
       "      <td>hays.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244766</th>\n",
       "      <td>72705216</td>\n",
       "      <td>SUPPLY TEACHERS</td>\n",
       "      <td>Position: Qualified Teacher Subject/Specialism...</td>\n",
       "      <td>Wiltshire</td>\n",
       "      <td>Wiltshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>contract</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teaching Jobs</td>\n",
       "      <td>450 to 500 per week</td>\n",
       "      <td>22800</td>\n",
       "      <td>hays.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244767</th>\n",
       "      <td>72705235</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>This entrepreneurial and growing private equit...</td>\n",
       "      <td>Hitchin</td>\n",
       "      <td>Hitchin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teaching Jobs</td>\n",
       "      <td>40-45,000</td>\n",
       "      <td>42500</td>\n",
       "      <td>hays.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244768 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                                              Title  \\\n",
       "0       12612628                        Engineering Systems Analyst   \n",
       "1       12612830                            Stress Engineer Glasgow   \n",
       "2       12612844                   Modelling and simulation analyst   \n",
       "3       12613049  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4       12613647         Pioneer, Miser Engineering Systems Analyst   \n",
       "...          ...                                                ...   \n",
       "244763  72705211                                 TEACHER OF SCIENCE   \n",
       "244764  72705212                TEACHER OF BUSINESS STUDIES AND ICT   \n",
       "244765  72705213                                    ENGLISH TEACHER   \n",
       "244766  72705216                                    SUPPLY TEACHERS   \n",
       "244767  72705235                                         Accountant   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "0       Engineering Systems Analyst Dorking Surrey Sal...   \n",
       "1       Stress Engineer Glasgow Salary **** to **** We...   \n",
       "2       Mathematical Modeller / Simulation Analyst / O...   \n",
       "3       Engineering Systems Analyst / Mathematical Mod...   \n",
       "4       Pioneer, Miser  Engineering Systems Analyst Do...   \n",
       "...                                                   ...   \n",
       "244763  Position: Qualified Teacher Subject/Specialism...   \n",
       "244764  Position: Qualified Teacher or NQT Subject/Spe...   \n",
       "244765  Position: Qualified Teacher Subject/Specialism...   \n",
       "244766  Position: Qualified Teacher Subject/Specialism...   \n",
       "244767  This entrepreneurial and growing private equit...   \n",
       "\n",
       "                              LocationRaw LocationNormalized ContractType  \\\n",
       "0                 Dorking, Surrey, Surrey            Dorking          NaN   \n",
       "1             Glasgow, Scotland, Scotland            Glasgow          NaN   \n",
       "2       Hampshire, South East, South East          Hampshire          NaN   \n",
       "3          Surrey, South East, South East             Surrey          NaN   \n",
       "4          Surrey, South East, South East             Surrey          NaN   \n",
       "...                                   ...                ...          ...   \n",
       "244763                            Swindon            Swindon          NaN   \n",
       "244764                            Swindon            Swindon          NaN   \n",
       "244765                            Swindon            Swindon          NaN   \n",
       "244766                          Wiltshire          Wiltshire          NaN   \n",
       "244767                            Hitchin            Hitchin          NaN   \n",
       "\n",
       "       ContractTime                       Company          Category  \\\n",
       "0         permanent  Gregory Martin International  Engineering Jobs   \n",
       "1         permanent  Gregory Martin International  Engineering Jobs   \n",
       "2         permanent  Gregory Martin International  Engineering Jobs   \n",
       "3         permanent  Gregory Martin International  Engineering Jobs   \n",
       "4         permanent  Gregory Martin International  Engineering Jobs   \n",
       "...             ...                           ...               ...   \n",
       "244763     contract                           NaN     Teaching Jobs   \n",
       "244764     contract                           NaN     Teaching Jobs   \n",
       "244765     contract                           NaN     Teaching Jobs   \n",
       "244766     contract                           NaN     Teaching Jobs   \n",
       "244767    permanent                           NaN     Teaching Jobs   \n",
       "\n",
       "                                     SalaryRaw  SalaryNormalized  \\\n",
       "0                   20000 - 30000/annum 20-30K             25000   \n",
       "1                   25000 - 35000/annum 25-35K             30000   \n",
       "2                   20000 - 40000/annum 20-40K             30000   \n",
       "3       25000 - 30000/annum 25K-30K negotiable             27500   \n",
       "4                   20000 - 30000/annum 20-30K             25000   \n",
       "...                                        ...               ...   \n",
       "244763                      450 - 500 per week             22800   \n",
       "244764                      450 - 500 per week             22800   \n",
       "244765                      450 - 500 per week             22800   \n",
       "244766                     450 to 500 per week             22800   \n",
       "244767                               40-45,000             42500   \n",
       "\n",
       "              SourceName  \n",
       "0       cv-library.co.uk  \n",
       "1       cv-library.co.uk  \n",
       "2       cv-library.co.uk  \n",
       "3       cv-library.co.uk  \n",
       "4       cv-library.co.uk  \n",
       "...                  ...  \n",
       "244763        hays.co.uk  \n",
       "244764        hays.co.uk  \n",
       "244765        hays.co.uk  \n",
       "244766        hays.co.uk  \n",
       "244767        hays.co.uk  \n",
       "\n",
       "[244768 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b1509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"data/jd_tokenizer_wordpiece.json\")\n",
    "tokenizer.enable_padding()\n",
    "tokenizer.enable_truncation(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8399fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8842e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=3, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=3, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_batch(['small sent', 'a larger sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2005afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class DataCollatorForLanguageModeling:\n",
    "    def __init__(self, tokenizer, mlm_probability=0.15):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm_probability = mlm_probability\n",
    "        self.special_token_ids = []\n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        for i in range(len(tokenizer.get_vocab())):\n",
    "            tok = tokenizer.id_to_token(i)\n",
    "            if tok[0] == '[' and tok[-1] == ']':\n",
    "                self.special_token_ids.append(i)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        shape = (len(batch), len(batch[0]))\n",
    "        labels = torch.full(shape, -100)\n",
    "        mask = torch.full(shape, True)\n",
    "#         input_ids\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[0])):\n",
    "                tok = batch[i][j]\n",
    "                if tok == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    r1 = random.random()\n",
    "                    r2 = random.random()\n",
    "                    if r1 < self.mlm_probability and tok not in self.special_token_ids:\n",
    "                        if r2 < 0.8:\n",
    "                            replacement = self.tokenizer.token_to_id('[MASK]')\n",
    "                        elif r2 < 0.9:\n",
    "                            replacement = random.randint(100, self.vocab_size-1)\n",
    "                        else:\n",
    "                            replacement = tok\n",
    "                        \n",
    "                        batch[i][j] = replacement\n",
    "                        labels[i][j] = tok\n",
    "                    else:\n",
    "                        mask[i][j] = False\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(batch),\n",
    "            'mask': mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6192fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "597fde3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlm_batch(sentences):\n",
    "    sentences = [f'[CLS] {s} [SEP]' for s in sentences]\n",
    "    toks = tokenizer.encode_batch(\n",
    "        sentences,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    toks = [t.ids for t in toks]\n",
    "    \n",
    "    return dc(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95b3784e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2,    4,  200,   37, 4357,    3,    0,    0],\n",
       "         [   2,  220,  200, 3966, 4357,  768, 4311,    3]]),\n",
       " 'mask': tensor([[False,  True, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False]]),\n",
       " 'labels': tensor([[-100,  220, -100, -100, -100, -100, -100, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mlm_batch(['this is a sent', 'this is another sent but longer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf8839d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_by_line_generator(jds, batch_size=32, max_length=128):\n",
    "    examples = []\n",
    "    ex = ''\n",
    "    current_token_count = 0\n",
    "    while True:\n",
    "        for jd in jds:\n",
    "            if len(ex) > 0:\n",
    "                ex += ' [SEP] '\n",
    "            sents = sent_tokenize(jd)\n",
    "            for sent in sents:\n",
    "                sent_len = len(sent.split())\n",
    "                if current_token_count + sent_len >= max_length:\n",
    "                    examples.append(ex)\n",
    "                    ex = ''\n",
    "                    current_token_count = 0\n",
    "                    if len(examples) == batch_size:\n",
    "                        batch = get_mlm_batch(examples)\n",
    "                        yield batch\n",
    "                        examples = []\n",
    "\n",
    "                ex += sent\n",
    "                current_token_count += sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d3a0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, batch in enumerate(line_by_line_generator(df.FullDescription, batch_size=4)):\n",
    "    if index > 5:\n",
    "        break\n",
    "#     print(len(batch))\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92fdee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9bc54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, emb_dimension, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.n_head = n_head\n",
    "        self.qw = nn.Linear(self.emb_dimension, self.emb_dimension)\n",
    "        self.kw = nn.Linear(self.emb_dimension, self.emb_dimension)\n",
    "        self.vw = nn.Linear(self.emb_dimension, self.emb_dimension)\n",
    "        self.proj = nn.Linear(self.emb_dimension, self.emb_dimension)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, V = x.shape\n",
    "        \n",
    "        q = self.qw(x).view(B, T, self.n_head, V // self.n_head).transpose(1, 2) # shape will be B, b_head, T, V // n_head\n",
    "        k = self.kw(x).view(B, T, self.n_head, V // self.n_head).transpose(1, 2)\n",
    "        v = self.vw(x).view(B, T, self.n_head, V // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        \n",
    "        # scaled dot attention\n",
    "        att = (q @ k.transpose(-1, -2)) * (1.0 / math.sqrt(k.size(-1))) # shape will be B, n_head, T, T\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, T, 1).unsqueeze(1)\n",
    "            att.masked_fill_(mask, -1e9)\n",
    "        \n",
    "        att = torch.softmax(att, -1)\n",
    "        out = att @ v\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        out = out.transpose(1, 2)\n",
    "        out = out.reshape(B, T, V)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return {\n",
    "            'output': out,\n",
    "            'mask': mask,\n",
    "            'attention': att\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ce54313",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "embedding_dim = 6\n",
    "timesteps = 4\n",
    "x = torch.rand((batch_size, timesteps, embedding_dim))\n",
    "mask = [[True if random.random() > 0.8 else False for _ in range(timesteps)] for _ in range(batch_size)]\n",
    "mask = torch.tensor(mask)\n",
    "sal = SelfAttentionLayer(embedding_dim, 2, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "156eaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sal(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f60341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dimension=512, n_head=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dimension)\n",
    "        self.ln2 = nn.LayerNorm(emb_dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.self_attention = SelfAttentionLayer(emb_dimension, n_head, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(emb_dimension, 4*emb_dimension),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*emb_dimension, emb_dimension),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.self_attention(self.ln1(x), mask)['output']\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edb98125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3641,  0.0990,  0.9504,  0.2047,  0.7247,  0.5811],\n",
       "         [-0.6783,  0.3565, -0.0195,  0.9071,  0.9127,  2.1209],\n",
       "         [ 0.6961,  0.8577,  0.4497,  0.5552,  0.9135,  1.8864],\n",
       "         [ 0.3561,  0.6649,  0.7294, -0.2813,  0.5614,  2.1200]],\n",
       "\n",
       "        [[-0.1444,  0.2995, -0.2673,  1.5892,  0.9499,  1.5313],\n",
       "         [-0.1647,  0.6111,  0.3787,  0.1242,  0.4752,  1.0457],\n",
       "         [ 0.2273, -0.0912,  1.2180,  0.9176,  0.8547,  1.1293],\n",
       "         [-0.5180,  0.0685,  1.6853,  0.0676,  0.7560,  1.3780]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eb = EncoderBlock(embedding_dim, 2, 0.0)\n",
    "eb(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c192ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_MLM(nn.Module):\n",
    "    def __init__(self,tokenizer=None, emb_dimension=512, n_head=8, dropout=0.1, n_layers=6):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_encoder = nn.Embedding(tokenizer.get_vocab_size(), emb_dimension)\n",
    "        self.position_encoder = nn.Parameter(torch.zeros(1, 512, emb_dimension).uniform_(0, 0.05))\n",
    "        \n",
    "        \n",
    "        self.encoders = nn.ModuleList([EncoderBlock(emb_dimension, n_head, dropout) for _ in range(n_layers)])\n",
    "        self.lm_head = nn.Linear(emb_dimension, tokenizer.get_vocab_size())\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B, T = x.shape\n",
    "        x_tok = self.token_encoder(x)\n",
    "        x_pos = self.position_encoder[:, :T, :]\n",
    "        \n",
    "        x_emb = x_tok + x_pos\n",
    "        \n",
    "        for block in self.encoders:\n",
    "            x_emb = block(x_emb, mask)\n",
    "            \n",
    "        out = self.lm_head(x_emb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "025f969f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer_MLM(\n",
       "  (token_encoder): Embedding(36000, 512)\n",
       "  (encoders): ModuleList(\n",
       "    (0): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=36000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = Transformer_MLM(tokenizer)\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bf6089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(transformer_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55cf2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9a265fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[220, 118]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"thiss\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7d8582d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this ##s'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([220, 118])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eaecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss at 0 is 5.0358940937915255\n",
      "Running loss at 25 is 4.73713755090494\n",
      "Running loss at 50 is 4.793367494528374\n",
      "Running loss at 75 is 4.705108485822459\n",
      "Running loss at 100 is 4.73905204824572\n",
      "Running loss at 125 is 4.910830560440085\n",
      "Running loss at 150 is 4.669873943266832\n",
      "Running loss at 175 is 4.781158874534393\n",
      "Running loss at 200 is 4.804618103764698\n",
      "Running loss at 225 is 4.749888803131924\n",
      "Running loss at 250 is 4.701550490534748\n",
      "Running loss at 275 is 4.7183380647396564\n",
      "Running loss at 300 is 4.74727362185067\n",
      "Running loss at 325 is 4.766252402355877\n",
      "Running loss at 350 is 4.836884506473715\n",
      "Running loss at 375 is 4.748392125066183\n",
      "Running loss at 400 is 4.775037403746823\n",
      "Running loss at 425 is 4.721591000983358\n",
      "Running loss at 450 is 4.768648720401286\n",
      "Running loss at 475 is 4.7026793678066925\n",
      "Running loss at 500 is 4.685111737477078\n",
      "Running loss at 525 is 4.748058804007624\n",
      "Running loss at 550 is 4.726923407599834\n",
      "Running loss at 575 is 4.756959985523662\n",
      "Running loss at 600 is 4.7793708889077875\n",
      "Running loss at 625 is 4.690739726490662\n",
      "Running loss at 650 is 4.766375711771341\n",
      "Running loss at 675 is 4.971678660096094\n",
      "Running loss at 700 is 4.807034118273676\n",
      "Running loss at 725 is 4.774201675773162\n",
      "Running loss at 750 is 4.657016055114744\n",
      "Running loss at 775 is 4.6749950025294496\n",
      "Running loss at 800 is 4.682343504727714\n",
      "Running loss at 825 is 4.708131260143307\n",
      "Running loss at 850 is 4.737754275882143\n",
      "Running loss at 875 is 4.532035353639012\n",
      "Running loss at 900 is 4.795583928875267\n",
      "Running loss at 925 is 4.638520928981546\n",
      "Running loss at 950 is 4.739390032838123\n",
      "Running loss at 975 is 4.739680557034449\n",
      "Running loss at 1000 is 4.7186522832622435\n",
      "Running loss at 1025 is 4.771484399928558\n",
      "Running loss at 1050 is 4.769706494714795\n",
      "Running loss at 1075 is 4.771113639986034\n",
      "Running loss at 1100 is 4.7822779433903255\n",
      "Running loss at 1125 is 4.745238437584483\n",
      "Running loss at 1150 is 4.706572512286753\n",
      "Running loss at 1175 is 4.701498544906666\n",
      "Running loss at 1200 is 4.812464062333353\n",
      "Running loss at 1225 is 4.719870524626857\n",
      "Running loss at 1250 is 4.571343689885867\n",
      "Running loss at 1275 is 4.677508810231448\n",
      "Running loss at 1300 is 4.762078340306874\n",
      "Running loss at 1325 is 4.68167085837854\n",
      "Running loss at 1350 is 4.755404130948549\n",
      "Running loss at 1375 is 4.671245275925367\n",
      "Running loss at 1400 is 4.624557529097596\n",
      "Running loss at 1425 is 4.695601161391516\n",
      "Running loss at 1450 is 4.583645701968454\n",
      "Running loss at 1475 is 4.499013884470651\n",
      "Running loss at 1500 is 4.702448840813335\n",
      "Running loss at 1525 is 4.67644841130312\n",
      "Running loss at 1550 is 4.618988093391318\n",
      "Running loss at 1575 is 4.634725902890471\n",
      "Running loss at 1600 is 4.671774238828161\n",
      "Running loss at 1625 is 4.684413889006568\n",
      "Running loss at 1650 is 4.724682019071085\n",
      "Running loss at 1675 is 4.693651253646105\n",
      "Running loss at 1700 is 4.663133286646798\n",
      "Running loss at 1725 is 4.510616002165456\n",
      "Running loss at 1750 is 4.756181250488025\n",
      "Running loss at 1775 is 4.573441537423236\n",
      "Running loss at 1800 is 4.572846654013584\n",
      "Running loss at 1825 is 4.581293319276852\n",
      "Running loss at 1850 is 4.602165941919579\n",
      "Running loss at 1875 is 4.631251837600872\n",
      "Running loss at 1900 is 4.703788679482076\n",
      "Running loss at 1925 is 4.750568494661492\n",
      "Running loss at 1950 is 4.567047909282986\n",
      "Running loss at 1975 is 4.543566834068799\n",
      "Running loss at 2000 is 4.732604312598154\n",
      "Running loss at 2025 is 4.648886671089415\n",
      "Running loss at 2050 is 4.598795472813259\n",
      "Running loss at 2075 is 4.613591046278079\n",
      "Running loss at 2100 is 4.583398928688081\n",
      "Running loss at 2125 is 4.537168084234026\n",
      "Running loss at 2150 is 4.667025735604394\n",
      "Running loss at 2175 is 4.517647045979112\n",
      "Running loss at 2200 is 4.595907242638148\n",
      "Running loss at 2225 is 4.522067607934959\n",
      "Running loss at 2250 is 4.516947660866354\n",
      "Running loss at 2275 is 4.413762469604176\n",
      "Running loss at 2300 is 4.487542013656053\n",
      "Running loss at 2325 is 4.646538277779289\n",
      "Running loss at 2350 is 4.663909506111368\n",
      "Running loss at 2375 is 4.606370569675464\n",
      "Running loss at 2400 is 4.596962287352128\n",
      "Running loss at 2425 is 4.544374916910631\n",
      "Running loss at 2450 is 4.5077017351747415\n",
      "Running loss at 2475 is 4.527489680470645\n",
      "Running loss at 2500 is 4.6922313112219385\n",
      "Running loss at 2525 is 4.635685974693361\n",
      "Running loss at 2550 is 4.465284116910424\n",
      "Running loss at 2575 is 4.479656079726019\n",
      "Running loss at 2600 is 4.561312706473385\n",
      "Running loss at 2625 is 4.661546128970533\n",
      "Running loss at 2650 is 4.651035333657308\n",
      "Running loss at 2675 is 4.523768480520842\n",
      "Running loss at 2700 is 4.564664327468183\n",
      "Running loss at 2725 is 4.4551775239273805\n",
      "Running loss at 2750 is 4.496105107305594\n",
      "Running loss at 2775 is 4.590863216895059\n",
      "Running loss at 2800 is 4.520991790594983\n",
      "Running loss at 2825 is 4.663834380600847\n",
      "Running loss at 2850 is 4.605650304918123\n",
      "Running loss at 2875 is 4.383235643758705\n",
      "Running loss at 2900 is 4.652000490383235\n",
      "Running loss at 2925 is 4.367444681766035\n",
      "Running loss at 2950 is 4.568505492030168\n",
      "Running loss at 2975 is 4.540254845149302\n",
      "Running loss at 3000 is 4.461693222398919\n",
      "Running loss at 3025 is 4.4082406715275315\n",
      "Running loss at 3050 is 4.458199913988454\n",
      "Running loss at 3075 is 4.55278211783921\n",
      "Running loss at 3100 is 4.548501064894886\n",
      "Running loss at 3125 is 4.669144139854693\n",
      "Running loss at 3150 is 4.412959519695395\n",
      "Running loss at 3175 is 4.548731149615332\n",
      "Running loss at 3200 is 4.488622711005411\n",
      "Running loss at 3225 is 4.657325460426258\n",
      "Running loss at 3250 is 4.465503808019438\n",
      "Running loss at 3275 is 4.594716856015718\n",
      "Running loss at 3300 is 4.643799296138272\n",
      "Running loss at 3325 is 4.4048247862316074\n",
      "Running loss at 3350 is 4.374716317444926\n"
     ]
    }
   ],
   "source": [
    "accumulation_steps = 8\n",
    "transformer_model.train()\n",
    "for index, batch in enumerate(line_by_line_generator(df.FullDescription.sample(frac=1.0), batch_size=8, max_length=128)):\n",
    "    if index > 50000:\n",
    "        break\n",
    "\n",
    "    inp = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    output = transformer_model(inp)\n",
    "    output_shape = output.shape\n",
    "    loss_all = F.cross_entropy(output.view(-1, output_shape[-1]), labels.view(-1), reduction='none')\n",
    "    loss = loss_all[loss_all > 0].mean()\n",
    "    lit = loss.item()\n",
    "    losses.append(lit)\n",
    "    running_loss = 0.9 * running_loss + 0.1 * lit\n",
    "    if index % 25 == 0:\n",
    "        print(f'Running loss at {index} is {running_loss}')\n",
    "    \n",
    "    loss = loss / accumulation_steps\n",
    "    loss.backward()\n",
    "    if (index+1) % accumulation_steps == 0:  \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f3f0de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(284, device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(loss_all > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a70886d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([1.0323e-04, 1.5722e-04, 9.6846e-04, 1.5454e-03, 3.1191e-03, 3.9472e-03,\n",
       "        1.6362e-02, 3.0010e-02, 3.2247e-02, 1.2260e-01, 1.5431e-01, 1.8654e-01,\n",
       "        2.7251e-01, 3.7736e-01, 3.8077e-01, 9.1883e-01, 1.0843e+00, 1.1882e+00,\n",
       "        1.5827e+00, 1.6409e+00, 1.7808e+00, 1.8598e+00, 1.9958e+00, 2.0038e+00,\n",
       "        2.0543e+00, 2.0925e+00, 2.1903e+00, 2.2966e+00, 2.3017e+00, 2.3953e+00,\n",
       "        2.4019e+00, 2.4454e+00, 2.5734e+00, 2.5958e+00, 2.6387e+00, 2.6436e+00,\n",
       "        2.6500e+00, 2.6778e+00, 2.6826e+00, 2.7486e+00, 2.8646e+00, 2.9105e+00,\n",
       "        2.9994e+00, 3.1524e+00, 3.1625e+00, 3.1645e+00, 3.1980e+00, 3.3089e+00,\n",
       "        3.4043e+00, 3.4289e+00, 3.5002e+00, 3.5002e+00, 3.5256e+00, 3.5302e+00,\n",
       "        3.5569e+00, 3.5624e+00, 3.5683e+00, 3.5840e+00, 3.5885e+00, 3.6162e+00,\n",
       "        3.6367e+00, 3.6766e+00, 3.6782e+00, 3.6893e+00, 3.7053e+00, 3.7538e+00,\n",
       "        3.7933e+00, 3.8246e+00, 3.9248e+00, 3.9422e+00, 3.9828e+00, 4.0101e+00,\n",
       "        4.1500e+00, 4.1673e+00, 4.1796e+00, 4.2001e+00, 4.2148e+00, 4.2278e+00,\n",
       "        4.2502e+00, 4.2892e+00, 4.2924e+00, 4.3143e+00, 4.3397e+00, 4.3968e+00,\n",
       "        4.4047e+00, 4.4255e+00, 4.4963e+00, 4.5130e+00, 4.5237e+00, 4.5584e+00,\n",
       "        4.6043e+00, 4.6899e+00, 4.7021e+00, 4.7225e+00, 4.7297e+00, 4.7365e+00,\n",
       "        4.7412e+00, 4.8384e+00, 4.8915e+00, 4.9546e+00, 5.0264e+00, 5.0319e+00,\n",
       "        5.0390e+00, 5.0741e+00, 5.0754e+00, 5.1240e+00, 5.1678e+00, 5.1733e+00,\n",
       "        5.1926e+00, 5.2625e+00, 5.2772e+00, 5.3529e+00, 5.3962e+00, 5.4056e+00,\n",
       "        5.4156e+00, 5.4662e+00, 5.4848e+00, 5.4925e+00, 5.4961e+00, 5.5180e+00,\n",
       "        5.5878e+00, 5.6092e+00, 5.6259e+00, 5.6416e+00, 5.6958e+00, 5.7660e+00,\n",
       "        5.7780e+00, 5.8217e+00, 5.8439e+00, 5.8540e+00, 5.8990e+00, 5.9422e+00,\n",
       "        5.9764e+00, 5.9773e+00, 5.9990e+00, 6.0308e+00, 6.1313e+00, 6.2596e+00,\n",
       "        6.2733e+00, 6.3122e+00, 6.3267e+00, 6.3818e+00, 6.4400e+00, 6.4568e+00,\n",
       "        6.4839e+00, 6.5372e+00, 6.5382e+00, 6.6083e+00, 6.6433e+00, 6.6516e+00,\n",
       "        6.6542e+00, 6.6882e+00, 6.6953e+00, 6.7190e+00, 6.7213e+00, 6.7509e+00,\n",
       "        6.8109e+00, 6.9100e+00, 6.9116e+00, 6.9207e+00, 6.9983e+00, 7.0809e+00,\n",
       "        7.0810e+00, 7.1119e+00, 7.1383e+00, 7.2074e+00, 7.2166e+00, 7.2370e+00,\n",
       "        7.2470e+00, 7.2531e+00, 7.3653e+00, 7.3774e+00, 7.3841e+00, 7.3903e+00,\n",
       "        7.4704e+00, 7.4809e+00, 7.5031e+00, 7.5035e+00, 7.5325e+00, 7.5479e+00,\n",
       "        7.5568e+00, 7.5887e+00, 7.5902e+00, 7.5920e+00, 7.6853e+00, 7.7198e+00,\n",
       "        7.7273e+00, 7.7451e+00, 7.7716e+00, 7.8356e+00, 7.8444e+00, 7.8823e+00,\n",
       "        7.8826e+00, 7.9148e+00, 7.9281e+00, 7.9757e+00, 7.9898e+00, 8.0562e+00,\n",
       "        8.0569e+00, 8.1089e+00, 8.2478e+00, 8.2921e+00, 8.3233e+00, 8.3537e+00,\n",
       "        8.3775e+00, 8.4049e+00, 8.4072e+00, 8.4732e+00, 8.4877e+00, 8.5765e+00,\n",
       "        8.5775e+00, 8.5792e+00, 8.6732e+00, 8.6749e+00, 8.6915e+00, 8.7314e+00,\n",
       "        8.7436e+00, 8.8269e+00, 8.8634e+00, 8.8788e+00, 8.9507e+00, 8.9509e+00,\n",
       "        8.9581e+00, 9.0237e+00, 9.1678e+00, 9.2241e+00, 9.2900e+00, 9.4825e+00,\n",
       "        9.4929e+00, 9.5064e+00, 9.5220e+00, 9.6154e+00, 9.6379e+00, 9.6983e+00,\n",
       "        9.7046e+00, 9.7181e+00, 9.7599e+00, 9.8925e+00, 1.0055e+01, 1.0082e+01,\n",
       "        1.0082e+01, 1.0084e+01, 1.0106e+01, 1.0159e+01, 1.0170e+01, 1.0213e+01,\n",
       "        1.0285e+01, 1.0324e+01, 1.0424e+01, 1.0541e+01, 1.0543e+01, 1.0547e+01,\n",
       "        1.0553e+01, 1.0564e+01, 1.0588e+01, 1.0689e+01, 1.0766e+01, 1.0846e+01,\n",
       "        1.0853e+01, 1.0874e+01, 1.0939e+01, 1.1060e+01, 1.1108e+01, 1.1126e+01,\n",
       "        1.1286e+01, 1.1359e+01, 1.1506e+01, 1.1525e+01, 1.1525e+01, 1.1529e+01,\n",
       "        1.1691e+01, 1.1851e+01, 1.1868e+01, 1.2227e+01, 1.2386e+01, 1.2733e+01,\n",
       "        1.2848e+01, 1.2882e+01, 1.3025e+01, 1.3362e+01, 1.3470e+01, 1.3566e+01,\n",
       "        1.5242e+01, 1.6005e+01], device='cuda:0', grad_fn=<SortBackward>),\n",
       "indices=tensor([ 21, 250, 124, 194, 207, 111, 262, 199, 147, 265, 152, 260, 278,   0,\n",
       "        209, 202, 275, 230,  60,  45,  56, 210, 223,  58,  30, 212,  53, 256,\n",
       "         31,  80, 174, 251, 215,  61,  19,  55, 168, 231, 156, 257, 181,  20,\n",
       "         14, 268, 208,  28,  38,  33, 173,  47, 249,  36,  37, 241, 280, 213,\n",
       "        218,  57,  51, 153, 204, 177,  24, 240, 266,  97, 273, 271,  50, 203,\n",
       "        211, 150, 178, 226,  76,  10, 238,  12, 237, 107,   9, 279,  39, 195,\n",
       "        175,   6,   8, 233, 200, 102, 216, 227,   7, 206,  89, 109, 254, 282,\n",
       "        229, 146,  13, 245,  15,  41,  46, 283, 274,  32,  42, 120,  43, 267,\n",
       "        197,  63,  17, 228, 263,  83,  59, 224,  26,  54, 165,  25, 163, 246,\n",
       "         16,   5, 225, 264,  70, 158, 261,  96, 247, 136, 100,   4,  48, 180,\n",
       "        179, 244,  69, 221,  23,   3, 242, 205, 164, 166, 113,  65, 167,  64,\n",
       "         44, 259, 258,   2, 255,  35, 239, 140,  40, 138, 122, 196, 198,  99,\n",
       "        186, 105, 234,  34, 132, 134, 115, 130, 236,  52, 232, 131,  73, 148,\n",
       "        253, 145, 248,  84, 272, 235, 243, 217, 187,  18, 190, 127, 159, 220,\n",
       "         71,  87,  79, 157, 116, 270,  78,  92,  94,  90, 214,  62, 114, 182,\n",
       "        108, 154,  29,   1, 144, 185,  22, 125, 106, 193,  91,  27, 135, 191,\n",
       "        160, 119, 252, 110, 176, 117, 172,  72, 142,  86, 112, 222, 219,  67,\n",
       "        277,  98,  75,  81,  74, 143,  66, 129, 189, 141, 162,  93, 121, 201,\n",
       "        133, 104, 149, 126, 184,  82,  95, 137, 151, 192, 269, 171, 101,  49,\n",
       "         88,  11,  68, 103, 188, 281,  77, 123, 170, 118, 128, 276, 139, 161,\n",
       "         85, 169, 155, 183], device='cuda:0'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_all[loss_all>0].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3838d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, PATH):\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "def load_model(PATH):\n",
    "    model = RNN_MLM(tokenizer)\n",
    "    model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2dd942da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(rnn_mlm_model, 'models/rnn_6_786_bi_wordpiece.pt')\n",
    "# rnn_mlm_model = load_model('models/rnn_4_bi_wordpiece.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "549106f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8004, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24077bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1442f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "from plotly import express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff40e6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = pd.Series(losses)\n",
    "ls.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d63a9631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer_MLM(\n",
       "  (token_encoder): Embedding(36000, 512)\n",
       "  (encoders): ModuleList(\n",
       "    (0): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): EncoderBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attention): SelfAttentionLayer(\n",
       "        (qw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (vw): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=36000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f3a2603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C Developer Belfast Salary up to ****k pa Our client, a leading edge Software Development Centre in Belfast requires C Developers to deliver key software products directly for their clients and for their business teams using the latest Microsoft technologies (.NET C, ASP.NET and SQL Server). Key Accountabilities • Design and develop cuttingedge software solutions, developed in C .Net with SQL as the back end data store. • Ensure all deliverables for projects are completed on time, to budget and within quality standards. • Assist the Project Manager in production of estimates for development activity. • Work with the Business and Data Analysts to drive the requirements forward. • Provide support and consultancy across all test phases of the project. • Participation in planning of software releases, their execution and postrelease activities. • Assist with maintenance and production support. • Support colleagues through advice and technical assistance. Key Technical Skills • A degree in IT or related subject and at least 2 years recent relevant experience in .NET software development within a commercial environment. • Technically excellent in C, ASP.NET, MVC, web development using Visual Studio, also strong working knowledge of SQL Server and TFS. • Experience of full software lifecycle and different methodologies. • Working knowledge of design patterns and practices. • Excellent time management skills and the proven ability and drive to meet deadlines. • Excellent teamworking skills, ability to work independently, show ownership and commitment to the success of the team. • Proven problem solving skills, analytical mind and a keen eye for detail. Hours of work: Core hours are 8am to 6pm Monday to Friday and you will be required to work 37.5 hours per week, timings will be agreed with your manager. Recruitment Direct is acting as a recruitment agency'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Title.notnull() & df.Title.str.contains('Software Developer')].iloc[0]['FullDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2220962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"Designation: Python Distributed [MASK] Engineer\n",
    "Educational Qualifications: B.Tech/M.Tech/MS/MCA\n",
    "Experience: 3-9 Years\n",
    "What you will do\n",
    "Apply your [MASK] set to fetch data from [MASK] online web sources, cleanse it and [MASK] APIs on top of it.\n",
    "Work with [MASK] databases (MongoDB) to store raw/document-based data.\n",
    "Develop a deep [MASK] of our vast data sources on the web and know exactly how, when, and which data to scrape, parse and store.\n",
    "Develop [MASK] for automating and maintaining a [MASK] flow of data from multiple sources.\n",
    "Work independently with little supervision to research and test [MASK] solutions.\n",
    "What you will need\n",
    "Strong coding [MASK] in Python and [MASK] Web Framework.\n",
    "Information retrieval – Web [MASK].\n",
    "Experience with NoSQL data storage like [MASK].\n",
    "Good knowledge of Asynchronous task [MASK] like Celery.\n",
    "Experience working with large scale [MASK] and storage.\n",
    "Knowledge of [MASK] with Node.js is a plus.\n",
    "Knowledge of [MASK] with various front end technologies and how various websites are built.\n",
    "Sound [MASK] of Asynchronous Programming in python like AsyncIO.\"\"\"\n",
    "# para = para.replace('\\n', ' ')\n",
    "toks = tokenizer.encode(f'[CLS] {para} [SEP]')\n",
    "toks = torch.tensor([toks.ids]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "895fcc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer_model(toks)\n",
    "output = output.argmax(dim=-1)[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "736b3d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'defendant responsibilities : python engineer : engineer educational qualifications : b . tech / m . tech / ms / / experience : 3 3 9 years what you will do apply your . set to and data from and online web sources , cleanse it and the apis on top of it . work with the databases ( mongodb ) to store raw / document and based data . develop a deep and of our vast data sources on the web and know exactly how , when , and which data to ande , andse and store . develop and for automating and maintaining a and flow of data from multiple sources . work independently with little supervision to research and test and solutions . what you will need strong coding be in python and be web framework . information a – web and . experience with web data storage like and . good knowledge of experience task and like ay . experience working with large scale and and storage . knowledge of and with css . js is a plus . knowledge of and with various front end technologies and how various websites are built . sound and of i programming in python like understanding ando . disru'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.numpy()).replace(' ##', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17cb1a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.FullDescription.str.contains('data science')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59d81602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Location  Leeds. My client, an established and growing IT company are currently recruiting for a Telemarketing Executive. This is an exciting new business sales opportunity to join a forward thinking organisation that continues to go from strength to strength. The Telemarketing Executive role offers an excellent platform for progression to Senior Account Manager. Role Profile: The Telemarketer function is a key position to feed the growth objectives of the organisation. The intention of this position is to drive new clients to a Workshop and/or contact new clients on a range of specific products and service offerings on a project by project basis. The position is a training ground to further graduate onto junior account manager role after approximately ****2 months. It is expected that you will be capable of using the phone and email to convey value of the workshop or project with a qualified decision maker. Key Responsibilities: Your main targets will include: Ability to make **** outbound calls per day To speak with a minimum of 10 decision maker contacts To meet specific project targets E.g. workshop **** delegate per day To meet a minimum of 1 hour call each day Product, industry and service knowledge goals Your consultative style and proactivity will enable you to understand customers business, identify their needs and then leverage this information to achieve the required goal.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.FullDescription[5500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21078c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32b5f5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "# del rnn_mlm_model\n",
    "# del optimizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.view(-1, output.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ccf4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn_mlm_model(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbad180",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
